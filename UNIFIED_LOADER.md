# Unified Data Loader Guide

## Overview

The reconciliation system uses a **unified data loader** (`UnifiedDataLoaderAgent`) that handles file-based, API-based, and hybrid loading through a single, maintainable class. This guide covers how to use the unified data loader effectively.

## Current Implementation

### Unified Data Loader Features
- `UnifiedDataLoaderAgent` - Handles all data loading scenarios âœ… **ACTIVE**
- **Single class** with consistent interface
- **Automatic fallback** and source detection
- **Cleaner codebase** with optimized merging
- **Enhanced features** and better error handling

## Usage Guide

### 1. Import the Unified Data Loader

```python
from crew.agents.unified_data_loader import UnifiedDataLoaderAgent
```

### 2. Initialize the Data Loader

```python
# File-based only
loader = UnifiedDataLoaderAgent(data_dir="data/")

# API-based only
loader = UnifiedDataLoaderAgent(api_config=api_config)

# Hybrid (both file and API)
loader = UnifiedDataLoaderAgent(data_dir="data/", api_config=api_config)
```

### 3. Load Data with Different Sources

```python
# File-based loading
df = loader.load_data(source="files")

# API-based loading
df = loader.load_data(source="api", trade_ids=trade_ids, date=date)

# Auto-detect best available source
df = loader.load_data(source="auto")

# Hybrid loading (both sources)
df = loader.load_data(source="hybrid", trade_ids=trade_ids, date=date)
```

### 4. Command Line Usage

```bash
# File-based
python pipeline.py --source files

# API-based
python pipeline.py --source api --api-config api_config.json

# Auto-detect (default)
python pipeline.py

# Hybrid
python pipeline.py --source hybrid --api-config api_config.json

# With specific trade IDs
python pipeline.py --source api --api-config api_config.json --trade-ids TRADE001 TRADE002

# With specific date
python pipeline.py --source api --api-config api_config.json --date 2024-01-15
```

## Data Source Options

### 1. File-based Loading (`"files"`)
- **Purpose**: Load data from Excel files
- **Requirements**: Excel files in the data directory
- **Performance**: 1,000-10,000 trades/second
- **Use case**: Local development, offline processing

### 2. API-based Loading (`"api"`)
- **Purpose**: Load data from REST API endpoints
- **Requirements**: Valid API configuration
- **Performance**: 100-1,000 trades/second (network dependent)
- **Use case**: Production environments, real-time data

### 3. Auto-detect (`"auto"`)
- **Purpose**: Automatically choose the best available source
- **Logic**: Tries API first, falls back to files if API fails
- **Performance**: Varies based on selected source
- **Use case**: Flexible deployment, automatic fallback

### 4. Hybrid Loading (`"hybrid"`)
- **Purpose**: Load from both sources and merge intelligently
- **Requirements**: Both file and API sources available
- **Performance**: 500-5,000 trades/second (optimized merging)
- **Use case**: Data validation, redundancy, comprehensive analysis

## Configuration

### API Configuration Format

```json
{
  "base_url": "https://api.example.com",
  "api_key": "your_api_key",
  "timeout": 30,
  "endpoints": {
    "old_pricing": "/api/v1/pricing/old",
    "new_pricing": "/api/v1/pricing/new",
    "trade_metadata": "/api/v1/trades/metadata",
    "funding_reference": "/api/v1/funding/reference"
  }
}
```

### Data Directory Structure

```
data/
â”œâ”€â”€ old_pricing.xlsx
â”œâ”€â”€ new_pricing.xlsx
â”œâ”€â”€ trade_metadata.xlsx
â””â”€â”€ funding_model_reference.xlsx
```

## Advanced Features

### 1. Error Handling
- **Automatic fallback**: API failures trigger file loading
- **Graceful degradation**: Missing files don't crash the system
- **Detailed logging**: Clear error messages for debugging
- **Data validation**: Quality checks for loaded data

### 2. Performance Optimization
- **Efficient merging**: Optimized pandas operations
- **Memory management**: Streamlined data processing
- **Caching**: Model persistence for faster inference
- **Batch processing**: Handle large datasets efficiently

### 3. Data Quality
- **Validation**: Check for required columns and data types
- **Cleaning**: Handle missing values and duplicates
- **Summary statistics**: Data quality reports
- **Error reporting**: Detailed validation results

## Integration Examples

### 1. Crew Builder Integration

```python
from crew.crew_builder import ReconciliationCrew

# Initialize with unified data loader
crew = ReconciliationCrew(data_dir="data/", api_config=api_config)

# Run with different sources
df = crew.run(source="files")           # File-based
df = crew.run(source="api")             # API-based
df = crew.run(source="auto")            # Auto-detect
df = crew.run(source="hybrid")          # Hybrid
```

### 2. Service Integration

```python
from services.data_service import DataService

# Initialize service with unified loader
service = DataService(data_dir="data/", api_config=api_config)

# Use unified loader methods
status = service.get_data_source_status()
api_status = service.get_api_status()
quality = service.validate_data_quality(df)
summary = service.get_data_summary(df)
```

### 3. Testing Integration

```python
from test_api_connection import test_api_connection

# Test API connectivity with unified loader
result = test_api_connection(api_config)
print(f"API Status: {result}")
```

## Performance Characteristics

### Processing Capabilities
- **Data Loading**: 1,000-10,000 trades/second (file-based)
- **ML Training**: 10,000-100,000 trades/second (LightGBM)
- **ML Prediction**: 50,000-500,000 trades/second
- **Report Generation**: 1,000-10,000 trades/second

### Scalability
- **Memory Usage**: ~1MB per 1,000 trades
- **Storage**: Excel files + SQLite database
- **Concurrent Processing**: Single-threaded (can be parallelized)
- **Data Size Limits**: Up to 1M trades per file

### Real-world Performance
- **Small (<1K trades)**: 1-5 seconds total processing
- **Medium (1K-10K trades)**: 5-30 seconds total processing
- **Large (10K-100K trades)**: 30 seconds-5 minutes total processing

## Best Practices

### 1. Source Selection
- **Development**: Use `"files"` for local testing
- **Production**: Use `"api"` for real-time data
- **Flexible**: Use `"auto"` for automatic fallback
- **Comprehensive**: Use `"hybrid"` for data validation

### 2. Configuration Management
- **Environment variables**: Store sensitive API keys
- **Configuration files**: Use JSON for API settings
- **Validation**: Test configurations before deployment
- **Documentation**: Keep configuration examples updated

### 3. Error Handling
- **Graceful degradation**: Handle missing data sources
- **Logging**: Implement comprehensive error logging
- **Monitoring**: Track data quality and performance
- **Alerting**: Set up notifications for critical failures

### 4. Performance Optimization
- **Batch processing**: Process data in chunks
- **Caching**: Cache frequently accessed data
- **Memory management**: Monitor memory usage
- **Parallel processing**: Consider multi-threading for large datasets

## Troubleshooting

### Common Issues

#### 1. Data Loading Failures
- **Check file paths**: Ensure Excel files exist in data directory
- **Verify API configuration**: Test API endpoints manually
- **Check permissions**: Ensure read access to files and API
- **Validate data format**: Confirm Excel file structure

#### 2. Performance Issues
- **Monitor memory usage**: Large datasets may require more RAM
- **Check network latency**: API calls may be slow
- **Optimize data size**: Consider data compression or filtering
- **Use appropriate source**: Choose based on data size and requirements

#### 3. Configuration Errors
- **Validate JSON format**: Check API configuration syntax
- **Test API connectivity**: Verify endpoints are accessible
- **Check authentication**: Ensure API keys are valid
- **Review error messages**: Look for specific configuration issues

## Support Resources

### Documentation
- **UNIFIED_LOADER_SUMMARY.md**: Detailed implementation analysis
- **CLEANUP_SUMMARY.md**: System architecture overview
- **API_REFERENCE.md**: Complete API documentation
- **ARCHITECTURE.md**: System design and components

### Testing
```python
# Test file-based loading
crew = ReconciliationCrew(data_dir="data/")
df = crew.run(source="files")

# Test API-based loading  
crew = ReconciliationCrew(api_config=api_config)
df = crew.run(source="api")

# Test auto-detection
crew = ReconciliationCrew(data_dir="data/", api_config=api_config)
df = crew.run(source="auto")

# Test hybrid loading
crew = ReconciliationCrew(data_dir="data/", api_config=api_config)
df = crew.run(source="hybrid")
```

---

**ðŸŽ¯ The unified data loader provides a single, powerful interface for all data loading scenarios with automatic fallback, performance optimization, and comprehensive error handling.** 